{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection con optuna\n",
    "\n",
    "#dataset di test\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import optuna\n",
    "from optuna.storages import RDBStorage\n",
    "\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#modelli\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import di X_train, X_test, y_train, y_test con pandas\n",
    "X_train = np.array(pd.read_excel('X_train_2.xlsx').iloc[:,8:])\n",
    "X_test = np.array(pd.read_excel('X_test_2.xlsx').iloc[:,8:])\n",
    "y_train = np.array(pd.read_excel('y_train_2.xlsx'))-1\n",
    "y_test = np.array(pd.read_excel('y_test_2.xlsx'))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adatto i dati\n",
    "y_test = y_test.reshape(-1,)\n",
    "y_train = y_train.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_di_prova = False\n",
    "if dataset_di_prova:\n",
    "    data = load_digits()\n",
    "    X= data.data\n",
    "    y = data.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provo il random forest\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='weighted') # il parametro average è per il multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params_random_forest(X_train, y_train, X_test, y_test):\n",
    "    model = RandomForestClassifier(n_jobs=-1)\n",
    "    study = optuna.create_study(direction='maximize',\n",
    "                                study_name = \"study_random_forest\",\n",
    "                                storage=RDBStorage(\"sqlite:///study_random_forest.db\"), \n",
    "                                load_if_exists=True\n",
    "                                )\n",
    "\n",
    "    def objective(trial):\n",
    "        # Definisci i parametri da ottimizzare per il modello\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 100)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 200)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "        max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "        bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "        model.set_params(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                         min_samples_leaf=min_samples_leaf, max_features=max_features, bootstrap=bootstrap)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        #utilizzo f1 score come metrica dato che accuracy non è adatta per dataset sbilanciati\n",
    "        #facendo il kfold validation\n",
    "        kfold = KFold(n_splits=10, shuffle=True)\n",
    "        scores = []\n",
    "        for train_index, test_index in kfold.split(X_test):\n",
    "           X_train_fold, X_test_fold = X_test[train_index], X_test[test_index]\n",
    "           y_train_fold, y_test_fold = y_test[train_index], y_test[test_index]\n",
    "           model.fit(X_train_fold, y_train_fold)\n",
    "           y_pred_fold = model.predict(X_test_fold)\n",
    "           scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))\n",
    "        return np.mean(scores)\n",
    "\n",
    "\n",
    "    study.optimize(objective, n_trials=num_trials)\n",
    "    trials_df = study.trials_dataframe()\n",
    "    return trials_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params_svm(X_train, y_train, X_test, y_test):\n",
    "    model = SVC()\n",
    "    study = optuna.create_study(direction='maximize',\n",
    "                                study_name = \"study_svm\",\n",
    "                                storage=RDBStorage(\"sqlite:///study_svm.db\"), \n",
    "                                load_if_exists=True\n",
    "                                )\n",
    "\n",
    "    def objective(trial):\n",
    "        # Definisci i parametri da ottimizzare per il modello\n",
    "        C = trial.suggest_float('C', 0.1, 1000)\n",
    "        kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "        degree = trial.suggest_int('degree', 1, 10)\n",
    "        gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "\n",
    "        model.set_params(C=C, kernel=kernel, degree=degree, gamma=gamma)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        #utilizzo f1 score come metrica dato che accuracy non è adatta per dataset sbilanciati\n",
    "        return f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    study.optimize(objective, n_trials=num_trials)\n",
    "    trials_df = study.trials_dataframe()\n",
    "    return trials_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params_xgb(X_train, y_train, X_test, y_test):\n",
    "    model = xgb.XGBClassifier(n_jobs=-1)\n",
    "    study = optuna.create_study(direction='maximize',\n",
    "                                study_name = \"study_xgb\",\n",
    "                                storage=RDBStorage(\"sqlite:///study_xgb.db\"), \n",
    "                                load_if_exists=True\n",
    "                                )\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Definisci i parametri da ottimizzare per il modello\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 100)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 1)\n",
    "        gamma = trial.suggest_float('gamma', 0.01, 1)\n",
    "        subsample = trial.suggest_float('subsample', 0.01, 1)\n",
    "        colsample_bytree = trial.suggest_float('colsample_bytree', 0.01, 1)\n",
    "        reg_alpha = trial.suggest_float('reg_alpha', 0.01, 1)\n",
    "        reg_lambda = trial.suggest_float('reg_lambda', 0.01, 1)\n",
    "        min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "        objective = trial.suggest_categorical('objective', ['binary:logistic', 'binary:logitraw', 'binary:hinge'])\n",
    "        booster = trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart'])\n",
    "        colsample_bytree = trial.suggest_float('colsample_bytree', 0.01, 1)\n",
    "        gamma = trial.suggest_float('gamma', 0.01, 1)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 100)\n",
    "        min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "        subsample = trial.suggest_float('subsample', 0.01, 1)\n",
    "\n",
    "        model.set_params(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, gamma=gamma,\n",
    "                         subsample=subsample, colsample_bytree=colsample_bytree, reg_alpha=reg_alpha,\n",
    "                         reg_lambda=reg_lambda, min_child_weight=min_child_weight, objective=objective, booster=booster)\n",
    "        model.fit(X_train, y_train)\n",
    "        #utilizzo f1 score come metrica dato che accuracy non è adatta per dataset sbilanciati\n",
    "        kfold = KFold(n_splits=10, shuffle=True)\n",
    "        scores = []\n",
    "        for train_index, test_index in kfold.split(X_test):\n",
    "           X_train_fold, X_test_fold = X_test[train_index], X_test[test_index]\n",
    "           y_train_fold, y_test_fold = y_test[train_index], y_test[test_index]\n",
    "           model.fit(X_train_fold, y_train_fold)\n",
    "           y_pred_fold = model.predict(X_test_fold)\n",
    "           scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))\n",
    "        return np.mean(scores)\n",
    "    \n",
    "    study.optimize(objective, n_trials=num_trials)\n",
    "    trials_df = study.trials_dataframe()\n",
    "    return trials_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_optuna(X_test,y_test,X_train,y_train):\n",
    "\n",
    "    # definisco i modelli da utilizzare e le funzioni per la ricerca dei parametri migliori\n",
    "    models ={\n",
    "        'RandomForest': find_best_params_random_forest,\n",
    "        'SVM': find_best_params_svm,\n",
    "        'xgb': find_best_params_xgb\n",
    "    }\n",
    "\n",
    "    print(\"--- model selection ---\")\n",
    "    print(\"Modelli utilizzati:\", models.keys())\n",
    "    \n",
    "    trials_dataframes = {}\n",
    "    print(\"Trovo i parametri migliori per ogni modello...\")\n",
    "    print(\"\\n\")\n",
    "    print(\"modello: RandomForest\")\n",
    "    trials_dataframes['RandomForest'] = models['RandomForest'](X_train, y_train, X_test, y_test)\n",
    "    print(\"\\n\")\n",
    "    print(\"modello: SVM\")\n",
    "    trials_dataframes['SVM'] = models['SVM'](X_train, y_train, X_test, y_test)\n",
    "    print(\"\\n\")\n",
    "    print(\"modello: xgb\")\n",
    "    trials_dataframes['xgb'] = models['xgb'](X_train, y_train, X_test, y_test)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"risultati model selection:\")\n",
    "    for model_name in models.keys():\n",
    "        print(\"modello:\", model_name)\n",
    "        print(trials_dataframes[model_name].sort_values(by='value', ascending=True).head(1))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection = model_selection_optuna(X_test,y_test,X_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
